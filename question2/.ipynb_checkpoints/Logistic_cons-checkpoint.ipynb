{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此文档主要叙述问题以及基础知识和答案叙述。  \n",
    "code代码在LogisticMini文件内，  \n",
    "其中run.py是sample运行脚本 Non_negative_cons 函数是问题1的sample code   \n",
    "数据使用的是 sklearn.load_breast_cancer 输出是;，logistic_mini.py是类似sklearn的LogisticRegression类，在fix新加了 constraints 参数用于处理 LR引入constraints的优化问题；\n",
    "其中函数 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题2 ：\n",
    "Question 2\n",
    "\n",
    "We are all very familiar with Logistic Regression and its optimization solution to minimize its cost function. But in reality, we may need some constraints. The following are two special constraints, please give: a) optimization solution; b) code of the solution process; c) sample code (you can use the sklearn data set or generate your own code)\n",
    "\n",
    "1) Non-negative constraint: All coefficients of LR are required to be non-negative;\n",
    "\n",
    "2) Order preserving constraint: The coefficient of LR is required to satisfy a1 >= a2 >= a3 ……\n",
    "\n",
    "One more question: If there is no constraint, is LR a global optimization algorithm or a local one? why? When constraints 1) or 2) are imposed, global or local? why?\n",
    "\n",
    "logistic regression with weights constraints(non-nagetive,desceding order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题分析：问题是定位为 在一定约束下，求解逻辑回归最优解  \n",
    "**小问题1** ： 参数非负，即参数都是正的  \n",
    "**小问题2** :  参数是有序的，满足 a1>=a2>=a3...  \n",
    "**小问题3** ：全局优化和局部优化\n",
    "\n",
    "\n",
    "分为两个部分：  \n",
    "1、关于LR的基础知识  \n",
    "2、三个问题的求解  \n",
    "\n",
    "\n",
    "\n",
    "解题总思路：定位目标函数，选择优化方法，借助scipy,numpy，sklearn 使用方法 L-BFGS-B TNC trust-constr\n",
    "构建 linearConstraints\n",
    "<img src=\"./math1.png\" style=\"zoom:50%\"/>\n",
    "\n",
    "小问题1 linearConstraints 构建为 \n",
    "<img src=\"para_postive.jpg\" style=\"zoom:20%\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、关于LR的基础知识\n",
    "## 1、目标函数\n",
    "棒极了！现在，我们需要编写代价函数来评估结果。\n",
    "代价函数：\n",
    "$J\\left( \\theta  \\right)=\\frac{1}{m}\\sum\\limits_{i=1}^{m}{[-{{y}^{(i)}}\\log \\left( {{h}_{\\theta }}\\left( {{x}^{(i)}} \\right) \\right)-\\left( 1-{{y}^{(i)}} \\right)\\log \\left( 1-{{h}_{\\theta }}\\left( {{x}^{(i)}} \\right) \\right)]}$  \n",
    "本文使用的是 sklearn内的y={-1,1}  \n",
    "\n",
    "<img src=\"loss_fun.png\" style=\"zoom:50%\"/>\n",
    "\n",
    "## gradient \n",
    "\n",
    "* 转化为向量化计算： $\\frac{1}{m} X^T( Sigmoid(X\\theta) - y )$  \n",
    "\n",
    "$$\\frac{\\partial J\\left( \\theta  \\right)}{\\partial {{\\theta }_{j}}}=\\frac{1}{m}\\sum\\limits_{i=1}^{m}{({{h}_{\\theta }}\\left( {{x}^{(i)}} \\right)-{{y}^{(i)}})x_{_{j}}^{(i)}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题1 Non-negative constraint\n",
    "\n",
    "## 1、optimization solution \n",
    "针对LR 在constraints下的优化，使用了scipy.optimize.minimize 模块，method主要使用了 L-BFGS-B，TNC优化方法，  \n",
    "\n",
    "**L-BFGS-B**  \n",
    "L-BFGS算法是牛顿法的改进  \n",
    "\n",
    "阶段分为牛顿法，BFGS，L-BFGS  \n",
    "牛顿法的本质是泰勒级数的二阶展开  \n",
    "BFGS:解决牛顿法求逆矩阵的困难，是通过迭代逼近逆矩阵的拟牛顿法  \n",
    "L-BFGS(L指limited memory)\n",
    "为了解决算法每次存储D矩阵，需要大内存的问题，只保存BFGS迭代计算中的后几次迭代  \n",
    "\n",
    "**TNC**  \n",
    "\n",
    "Minimize a function with variables subject to bounds, using gradient information in a truncated Newton algorithm\n",
    "\n",
    "## bounds \n",
    "a1>=0,a2>=0,a3>=0....  \n",
    "上线边界即   \n",
    "  lb_ = np.zeros((X.shape[1]+1))  \n",
    "  ub_ = np.full(X.shape[1]+1, np.inf)\n",
    "## 2、sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mLogisticMini\u001b[m\u001b[m     __init__.py      para_postive.jpg\r\n",
      "Untitled.ipynb   math1.png        untitled.txt\r\n"
     ]
    }
   ],
   "source": [
    "def Non_negative_cons():\n",
    "\n",
    "    X, y = load_breast_cancer(return_X_y=True)\n",
    "    # sklearn y = {-1,1}\n",
    "    y[y==0] = -1\n",
    "    lb_ = np.zeros((X.shape[1]+1))\n",
    "\n",
    "    ub_ = np.full(X.shape[1]+1, np.inf)\n",
    "\n",
    "    bounds = Bounds(lb_, ub_)\n",
    "    print('lb_ {}'.format(lb_.shape))\n",
    "    print('ub_ {}'.format(ub_.shape))\n",
    "\n",
    "    ############\n",
    "    ####  method 可选 L-BFGS-B and TNC\n",
    "    ####  solver='lxw' 调用 _fix_lxw 执行minimize\n",
    "    ####  max_iter 迭代次数\n",
    "    #### fix_initial 是否更改初始值\n",
    "    #### bounds 设置 constraints 边界，问题1 即 a1>=0,a2>=2,a3>=3....\n",
    "\n",
    "    #   return param\n",
    "    #   max_iter ,penalty（none 即没用正则）\n",
    "    #   score 分类准确率\n",
    "    #   coef_ coefficient\n",
    "    #\n",
    "    ############\n",
    "    method = \"L-BFGS-B\"\n",
    "\n",
    "    clf = LogisticRegression(solver=\"lxw\", penalty=\"none\",max_iter=150)\n",
    "    # fix_initial 初值赋值更改，验证参数收敛是否一致，更改\n",
    "#     clf = LogisticRegression(solver=\"lxw\", penalty=\"none\",max_iter=60,fix_initial=True)\n",
    "\n",
    "    clf.fit(X, y, method=method,bounds=bounds)\n",
    "    print('===================== {} ==========='.format(method))\n",
    "    print('max_iter : {}'.format(clf.max_iter))\n",
    "    print('penalty : {}'.format(clf.penalty))\n",
    "\n",
    "    print('score : {}'.format(clf.score(X, y)))\n",
    "    print('intercept : {}'.format(clf.intercept_))\n",
    "    print('coef_  : {}'.format(clf.coef_))\n",
    "    print('res: {}  optimizer : {}'.format(method, clf.res.success))\n",
    "    \n",
    "    # L-BFGS-B 迭代 50、60次，参数相同，有最优解，\n",
    "    # == == == == == == == == == == = L - BFGS - B == == == == == =\n",
    "    # max_iter: 50\n",
    "    # penalty: none\n",
    "    # score: 0.6274165202108963\n",
    "    # intercept: [0.1967258]\n",
    "    # coef_: [[0.         0.         0.         0.         0.00889884 0.\n",
    "    #          0.         0.         0.0174621  0.01259587 0.         0.23684945\n",
    "    #          0.         0.         0.00180309 0.         0.         0.\n",
    "    #          0.00418088 0.00033521 0.         0.         0.         0.\n",
    "    #          0.00669678 0.         0.         0.         0.00518067 0.00472869]]\n",
    "    # res: L - BFGS - B\n",
    "    # optimizer: True\n",
    "\n",
    "    # ===================== L-BFGS-B ===========\n",
    "    # max_iter : 60\n",
    "    # penalty : none\n",
    "    # score : 0.6274165202108963\n",
    "    # intercept : [0.19999506]\n",
    "    # coef_  : [[0.         0.         0.         0.         0.         0.\n",
    "    #   0.         0.         0.         0.0020697  0.         0.24097333\n",
    "    #   0.         0.         0.00400276 0.         0.         0.\n",
    "    #   0.00099565 0.01133466 0.         0.         0.         0.\n",
    "    #   0.         0.         0.         0.         0.         0.        ]]\n",
    "    # res: L-BFGS-B  optimizer : True\n",
    "    \n",
    "    # 加入 l2的结果\n",
    "    # == == == == == == == == == == = L - BFGS - B == == == == == =\n",
    "    # max_iter: 50\n",
    "    # penalty: l2\n",
    "    # score: 0.6274165202108963\n",
    "    # intercept: [0.19640032]\n",
    "    # coef_: [[0.         0.         0.         0.         0.00888393 0.\n",
    "    #          0.         0.         0.01743283 0.01257477 0.         0.23645251\n",
    "    #          0.         0.         0.00180007 0.         0.         0.\n",
    "    #          0.00417387 0.00033464 0.         0.         0.         0.\n",
    "    #          0.00668555 0.         0.         0.         0.00517197 0.00472076]]\n",
    "    # res: L - BFGS - B\n",
    "    # optimizer: True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ===================== TNC ===========\n",
    "    # max_iter : 40\n",
    "    # penalty : none\n",
    "    # score : 0.6274165202108963\n",
    "    # intercept : [0.13340021]\n",
    "    # coef_  : [[ 0.          0.          0.          0.          0.          0.\n",
    "    #    0.          0.          0.          1.1488147   0.          0.\n",
    "    #    0.          0.         45.21328606  0.          0.          0.\n",
    "    #    0.          0.          0.          0.          0.          0.\n",
    "    #    0.          0.          0.          0.          0.          0.        ]]\n",
    "    # res: TNC  optimizer : False\n",
    "    \n",
    "    \n",
    "    #####  =========== 迭代50次以后优化器退出，为true，coef_稳定全局解\n",
    "    # ===================== TNC ===========\n",
    "    # max_iter : 50\n",
    "    # penalty : none\n",
    "    # score : 0.6274165202108963\n",
    "    # intercept : [0.17889717]\n",
    "    # coef_  : [[ 0.         0.         0.         0.         0.         0.\n",
    "    #    0.         0.         0.         0.         0.         0.\n",
    "    #    0.         0.        49.0536372  0.         0.         0.\n",
    "    #    0.         0.         0.         0.         0.         0.\n",
    "    #    0.         0.         0.         0.         0.         0.       ]]\n",
    "    # res: TNC  optimizer : True\n",
    "\n",
    "    # ===================== TNC ===========\n",
    "    # max_iter : 60\n",
    "    # penalty : none\n",
    "    # score : 0.6274165202108963\n",
    "    # intercept : [0.17889717]\n",
    "    # coef_  : [[ 0.         0.         0.         0.         0.         0.\n",
    "    #    0.         0.         0.         0.         0.         0.\n",
    "    #    0.         0.        49.0536372  0.         0.         0.\n",
    "    #    0.         0.         0.         0.         0.         0.\n",
    "    #    0.         0.         0.         0.         0.         0.       ]]\n",
    "    # res: TNC  optimizer : True\n",
    "    \n",
    "    ##### ====== 加入L2正则 迭代70次 收敛\n",
    "    # ===================== TNC ===========\n",
    "    # max_iter : 70\n",
    "    # penalty : l2\n",
    "    # score : 0.6274165202108963\n",
    "    # intercept : [0.48239063]\n",
    "    # coef_  : [[0.         0.         0.         0.         0.         0.\n",
    "    #   0.         0.         0.         0.02211936 0.         0.0303164\n",
    "    #   0.         0.         0.05256518 0.         0.         0.\n",
    "    #   0.0070915  0.         0.         0.         0.         0.\n",
    "    #   0.         0.         0.         0.         0.         0.        ]]\n",
    "    # res: TNC  optimizer : True\n",
    "\n",
    "    \n",
    "    ### 更改赋初值，coef_收敛相同值，迭代次数增加到150，LR 加入 Non-negative constraint 使用TNC 有全局最优解\n",
    "    # ===================== TNC ===========\n",
    "    # max_iter : 150\n",
    "    # penalty : none\n",
    "    # score : 0.6274165202108963\n",
    "    # intercept : [0.17889717]\n",
    "    # coef_  : [[ 0.          0.          0.          0.          0.          0.\n",
    "    #    0.          0.          0.          0.          0.          0.\n",
    "    #    0.          0.         49.05363767  0.          0.          0.\n",
    "    #    0.          0.          0.          0.          0.          0.\n",
    "    #    0.          0.          0.          0.          0.          0.        ]]\n",
    "    # res: TNC  optimizer : True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
